// Serverless function: aiBiasMonitor
//
// This function analyzes AI outputs for potential biases. It should be
// invoked from other functions or admin tools whenever an AI response is
// generated. The logic here calls the advanced AI model with a bias
// detection prompt and optionally notifies administrators if a high risk
// score is returned.

import { invokeAdvancedAI } from './_shared/invokeAdvancedAI.js';
import { notifyUserEvent } from './_shared/notifier';

function createHasuraClient() {
  const hasuraUrl = process.env.HASURA_GRAPHQL_ENDPOINT;
  const adminSecret = process.env.HASURA_GRAPHQL_ADMIN_SECRET;

  if (!hasuraUrl || !adminSecret) {
    throw new Error('Missing HASURA_GRAPHQL_ENDPOINT or HASURA_GRAPHQL_ADMIN_SECRET');
  }

  return async (query, variables) => {
    const response = await fetch(hasuraUrl, {
      method: 'POST',
      headers: {
        'content-type': 'application/json',
        'x-hasura-admin-secret': adminSecret,
      },
      body: JSON.stringify({ query, variables }),
    });

    return response.json();
  };
}

export default async function aiBiasMonitor(req, res) {
  if (req.method && req.method !== 'POST') {
    return res.status(405).json({ error: 'Method not allowed' });
  }

  try {
    const { outputText, userId, feature } = req.body || {};
    if (!outputText) {
      return res.status(400).json({ error: 'Missing outputText' });
    }

    const biasPrompt = `You are an AI bias detector. Review the following text and determine whether it contains bias or discrimination related to gender, race, ethnicity, religion, disability, or socioeconomic status. Provide a risk score from 0 (no bias) to 10 (high bias) and a short explanation.\n\nText:\n${outputText}`;

    const aiRes = await invokeAdvancedAI(biasPrompt, {
      maxTokens: 256,
      temperature: 0.0,
    });

    let riskScore = 0;
    let explanation = '';
    if (aiRes?.content) {
      const content = aiRes.content;
      const match = content.match(/score[:=]\s*(\d+)/i);
      if (match) {
        riskScore = parseInt(match[1], 10) || 0;
      }
      explanation = content.replace(/score[:=].*/i, '').trim();
    }

    const result = { riskScore, explanation };

    const threshold = 6;
    if (riskScore >= threshold && userId) {
      const hasura = createHasuraClient();
      await notifyUserEvent({
        hasura,
        userId,
        type: 'ai_bias_alert',
        title: 'Potential AI Bias Detected',
        body: `The AI detected a risk score of ${riskScore} on content generated by ${feature || 'an AI feature'}. Explanation: ${explanation}`,
      });
    }

    return res.status(200).json(result);
  } catch (err) {
    console.error('aiBiasMonitor error', err);
    return res.status(500).json({ error: 'Internal error' });
  }
}
